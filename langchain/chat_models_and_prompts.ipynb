{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4b014fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug 18 17:17:43 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.65.06              Driver Version: 580.65.06      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3070 ...    Off |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   63C    P0             32W /  140W |     699MiB /   8192MiB |     28%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            2690      G   /usr/libexec/Xorg                       186MiB |\n",
      "|    0   N/A  N/A            2794      G   /usr/bin/gnome-shell                    184MiB |\n",
      "|    0   N/A  N/A            3431      G   ...ersion=20250815-130033.239000        145MiB |\n",
      "|    0   N/A  N/A            3969      G   /usr/share/code/code                    145MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6fd309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import (\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf3e340",
   "metadata": {},
   "source": [
    "## Load chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57852b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a12fa5a778e043cdb635b4a393e63319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'min_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'min_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"janhq/Jan-v1-4B\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs=dict(max_new_tokens=4096),\n",
    "    model_kwargs={\"quantization_config\": quantization_config},\n",
    ")\n",
    "\n",
    "chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686fccb5",
   "metadata": {},
   "source": [
    "## Try chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a25616c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You're a Starwars hardcore fans. Answer concisely.<|im_end|>\n",
      "<|im_start|>user\n",
      "What happens when an unstoppable force meets an immovable object?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "Okay, the user is asking about the classic physics paradox: \"What happens when an unstoppable force meets an immovable object?\" They specifically mention being a Star Wars hardcore fan, so I need to tie this in with my knowledge of the franchise. \n",
      "\n",
      "Hmm, this is a philosophical question from real physics (or rather, the thought experiment), but the user wants a Star Wars twist. They're probably expecting humor or lore-based humor since they're hardcore. I recall that in Star Wars, the Force is often described as \"unstoppable\" and \"immovable\" in different contextsâ€”like the Force itself being a mystical energy field that can't be resisted, or objects being \"immovable\" through the Force.\n",
      "\n",
      "Wait, the user said \"concisely,\" so I shouldn't overexplain. They'd want something punchy that feels authentic to Star Wars. Maybe reference a specific scene or quote? Like, in *The Force Awakens*, Rey says \"I'm not afraid. What if it's not real?\" but that's not it... \n",
      "\n",
      "Ah! The best angle: in Star Wars, there's no true \"unstoppable force\" or \"immovable object\" because the Force makes everything relative. For example, the Death Star is \"unstoppable\" until it's destroyed, and planets are \"immovable\" until they're moved by the Force. So the answer could be that they don't meet because the Force resolves itâ€”like the Force *is* the unstoppable force and the immovable object. \n",
      "\n",
      "I should phrase it as a joke: \"In Star Wars, the Force is both. So they never meetâ€”they just... *do*.\" That's concise and fun. \n",
      "\n",
      "Also, the user might be trolling or testing if I know Star Wars lore deeply. Better avoid real physics jargon to keep it light. Final answer: lean into the Force as the solution.\n",
      "</think>\n",
      "\n",
      "In Star Wars lore, the Force *is* the unstoppable force and the immovable object simultaneously. So they never collideâ€”they just... *happen*. (Example: The Force moves objects without resistance, yet itâ€™s also \"unstoppable\" by definition.) ðŸŒŒ\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You're a Starwars hardcore fans. Answer concisely.\"),\n",
    "    HumanMessage(\n",
    "        content=\"What happens when an unstoppable force meets an immovable object?\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "ai_msg = chat_model.invoke(messages)\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63344dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You're a physics professor. Answer concisely.<|im_end|>\n",
      "<|im_start|>user\n",
      "What happens when an unstoppable force meets an immovable object?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "Okay, the user is asking about the classic \"unstoppable force meets immovable object\" paradox. Hmm, they specifically want me to act as a physics professor and keep it concise. \n",
      "\n",
      "First I should recall this is a philosophical thought experiment, not real physics. The user might be mixing up pop culture with actual science - maybe they heard it in a movie or book. I need to clarify it's hypothetical since both concepts are impossible in reality. \n",
      "\n",
      "*checks mental notes* Right, in Newtonian mechanics you can't have true unstoppable forces or immovable objects because forces always interact. The paradox only makes sense if you assume absolute concepts exist, which they don't. \n",
      "\n",
      "User's probably expecting a punchy answer, so I'll lead with \"it's a paradox\" to be precise. Should mention conservation laws too - that's the real physics angle they might not know. Oh! And emphasize that both terms are meaningless in standard physics to avoid confusion. \n",
      "\n",
      "Wait, they said \"concisely\" so I'll skip the history of the paradox (like it dates back to 19th century philosophy). Just give the cleanest explanation: no such things exist, so no collision happens. \n",
      "\n",
      "*imagines user's possible confusion* Maybe they're a student who needs this for homework? Better make sure I don't imply this is a real scenario. Add that \"no physical outcome\" line to be extra clear. \n",
      "\n",
      "Final structure: 1) It's not real 2) Why (conservation laws) 3) What actually happens if you try to force it (they interact normally). Keep it under 4 sentences like they want.\n",
      "</think>\n",
      "\n",
      "This is a philosophical paradox with no physical resolution, as **true unstoppable forces and immovable objects do not exist in physics**. In reality, all forces and objects obey conservation laws (e.g., momentum). If two objects collide, they interact via Newton's third law: neither is \"unstoppable\" nor \"immovable.\" The outcome follows standard mechanicsâ€”e.g., mutual acceleration or deformation. The premise itself is unphysical.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You're a physics professor. Answer concisely.\"),\n",
    "    HumanMessage(\n",
    "        content=\"What happens when an unstoppable force meets an immovable object?\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "ai_msg = chat_model.invoke(messages)\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93be0e01",
   "metadata": {},
   "source": [
    "## Try prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2456595",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"Translate the following from English into {language}\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "705124a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = prompt_template.invoke({\"language\": \"Italian\", \"text\": \"hi!\"})\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a630fbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "Translate the following from English into Italian<|im_end|>\n",
      "<|im_start|>user\n",
      "hi!<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "Okay, the user wants me to translate \"hi!\" from English to Italian. Let me think.\n",
      "\n",
      "First, \"hi\" is a casual greeting. In Italian, common greetings are \"ciao\" for \"hello\" or \"buongiorno\" for \"good day\". But \"hi\" is informal, so maybe \"ciao\" is the right choice here.\n",
      "\n",
      "Wait, \"ciao\" is used in many contexts, like \"ciao\" as a friendly \"hi\" or \"hello\". Let me check if there's a more precise term. For example, \"buonasera\" is \"good evening\", but \"hi\" is just a greeting. So \"ciao\" is probably the translation.\n",
      "\n",
      "I need to make sure. Let's recall some Italian phrases. Yes, \"ciao\" is the equivalent of \"hi\" or \"hello\" in informal Italian. So the translation would be \"ciao!\" with the exclamation mark to match the English.\n",
      "\n",
      "I don't think there's a different word for \"hi\" like \"hola\" in Spanish. In Italian, it's straightforward. So the answer should be \"ciao!\".\n",
      "\n",
      "Wait, sometimes people say \"ciao\" as a \"bye\" too, but the user wrote \"hi!\", so context here is greeting, so \"ciao\" is correct.\n",
      "\n",
      "Yes, I'll go with \"ciao!\".\n",
      "</think>\n",
      "\n",
      "ciao!\n"
     ]
    }
   ],
   "source": [
    "response = chat_model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd992f95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
